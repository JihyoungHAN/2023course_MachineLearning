{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "i= 0 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "i= 600 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 1200 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 1800 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 2400 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 3000 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 3600 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 4200 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 4800 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 5400 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 6000 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 6600 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 7200 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "i= 7800 train acc, test acc | 0.14593333333333333, 0.1462\n",
      "i= 8400 train acc, test acc | 0.18661666666666665, 0.1865\n",
      "i= 9000 train acc, test acc | 0.20498333333333332, 0.2048\n",
      "i= 9600 train acc, test acc | 0.27255, 0.2706\n",
      "i= 10200 train acc, test acc | 0.29635, 0.2978\n",
      "i= 10800 train acc, test acc | 0.3421166666666667, 0.3448\n",
      "i= 11400 train acc, test acc | 0.3736333333333333, 0.3762\n",
      "i= 12000 train acc, test acc | 0.3885, 0.391\n",
      "i= 12600 train acc, test acc | 0.4014, 0.4027\n",
      "i= 13200 train acc, test acc | 0.4084, 0.409\n",
      "i= 13800 train acc, test acc | 0.4186, 0.4186\n",
      "i= 14400 train acc, test acc | 0.42688333333333334, 0.4272\n",
      "i= 15000 train acc, test acc | 0.43901666666666667, 0.4397\n",
      "i= 15600 train acc, test acc | 0.44906666666666667, 0.4521\n",
      "i= 16200 train acc, test acc | 0.4592833333333333, 0.4616\n",
      "i= 16800 train acc, test acc | 0.4653833333333333, 0.4705\n",
      "i= 17400 train acc, test acc | 0.46913333333333335, 0.4737\n",
      "i= 18000 train acc, test acc | 0.4696166666666667, 0.4756\n",
      "i= 18600 train acc, test acc | 0.47281666666666666, 0.4786\n",
      "i= 19200 train acc, test acc | 0.47436666666666666, 0.4799\n",
      "i= 19800 train acc, test acc | 0.4751166666666667, 0.4815\n",
      "i= 20400 train acc, test acc | 0.47655, 0.4817\n",
      "i= 21000 train acc, test acc | 0.4769333333333333, 0.4816\n",
      "i= 21600 train acc, test acc | 0.4777166666666667, 0.4821\n",
      "i= 22200 train acc, test acc | 0.47815, 0.4825\n",
      "i= 22800 train acc, test acc | 0.47831666666666667, 0.4834\n",
      "i= 23400 train acc, test acc | 0.4789333333333333, 0.483\n",
      "i= 24000 train acc, test acc | 0.4794833333333333, 0.484\n",
      "i= 24600 train acc, test acc | 0.4797, 0.4843\n",
      "i= 25200 train acc, test acc | 0.48038333333333333, 0.4849\n",
      "i= 25800 train acc, test acc | 0.4811166666666667, 0.4853\n",
      "i= 26400 train acc, test acc | 0.48115, 0.4852\n",
      "i= 27000 train acc, test acc | 0.4812166666666667, 0.4847\n",
      "i= 27600 train acc, test acc | 0.48215, 0.4868\n",
      "i= 28200 train acc, test acc | 0.4816666666666667, 0.4853\n",
      "i= 28800 train acc, test acc | 0.4822166666666667, 0.4867\n",
      "i= 29400 train acc, test acc | 0.4827166666666667, 0.4866\n",
      "i= 30000 train acc, test acc | 0.48251666666666665, 0.486\n",
      "i= 30600 train acc, test acc | 0.4825, 0.4866\n",
      "i= 31200 train acc, test acc | 0.48281666666666667, 0.4871\n",
      "i= 31800 train acc, test acc | 0.4834, 0.4873\n",
      "i= 32400 train acc, test acc | 0.48365, 0.488\n",
      "i= 33000 train acc, test acc | 0.4839333333333333, 0.4879\n",
      "i= 33600 train acc, test acc | 0.48396666666666666, 0.4879\n",
      "i= 34200 train acc, test acc | 0.48423333333333335, 0.4883\n",
      "i= 34800 train acc, test acc | 0.48446666666666666, 0.4877\n",
      "i= 35400 train acc, test acc | 0.4844333333333333, 0.4874\n",
      "i= 36000 train acc, test acc | 0.4849, 0.488\n",
      "i= 36600 train acc, test acc | 0.4848, 0.4883\n",
      "i= 37200 train acc, test acc | 0.4844, 0.4877\n",
      "i= 37800 train acc, test acc | 0.4854, 0.4886\n",
      "i= 38400 train acc, test acc | 0.4857166666666667, 0.489\n",
      "i= 39000 train acc, test acc | 0.48606666666666665, 0.4889\n",
      "i= 39600 train acc, test acc | 0.48623333333333335, 0.4893\n",
      "i= 40200 train acc, test acc | 0.4865, 0.4895\n",
      "i= 40800 train acc, test acc | 0.4865333333333333, 0.4891\n",
      "i= 41400 train acc, test acc | 0.48665, 0.4895\n",
      "i= 42000 train acc, test acc | 0.48756666666666665, 0.4896\n",
      "i= 42600 train acc, test acc | 0.48773333333333335, 0.4901\n",
      "i= 43200 train acc, test acc | 0.48765, 0.4899\n",
      "i= 43800 train acc, test acc | 0.48746666666666666, 0.4904\n",
      "i= 44400 train acc, test acc | 0.4885333333333333, 0.4907\n",
      "i= 45000 train acc, test acc | 0.48886666666666667, 0.4906\n",
      "i= 45600 train acc, test acc | 0.48905, 0.4919\n",
      "i= 46200 train acc, test acc | 0.4887666666666667, 0.4912\n",
      "i= 46800 train acc, test acc | 0.489, 0.492\n",
      "i= 47400 train acc, test acc | 0.4890333333333333, 0.4915\n",
      "i= 48000 train acc, test acc | 0.48925, 0.4911\n",
      "i= 48600 train acc, test acc | 0.4894833333333333, 0.4923\n",
      "i= 49200 train acc, test acc | 0.48946666666666666, 0.4922\n",
      "i= 49800 train acc, test acc | 0.4895833333333333, 0.4918\n",
      "i= 50400 train acc, test acc | 0.48993333333333333, 0.4925\n",
      "i= 51000 train acc, test acc | 0.49016666666666664, 0.4929\n",
      "i= 51600 train acc, test acc | 0.49023333333333335, 0.4923\n",
      "i= 52200 train acc, test acc | 0.4894833333333333, 0.4927\n",
      "i= 52800 train acc, test acc | 0.49043333333333333, 0.4926\n",
      "i= 53400 train acc, test acc | 0.49041666666666667, 0.4924\n",
      "i= 54000 train acc, test acc | 0.4906, 0.4923\n",
      "i= 54600 train acc, test acc | 0.49025, 0.4931\n",
      "i= 55200 train acc, test acc | 0.4898, 0.4929\n",
      "i= 55800 train acc, test acc | 0.49046666666666666, 0.4927\n",
      "i= 56400 train acc, test acc | 0.49061666666666665, 0.4922\n",
      "i= 57000 train acc, test acc | 0.49143333333333333, 0.4923\n",
      "i= 57600 train acc, test acc | 0.49145, 0.4925\n",
      "i= 58200 train acc, test acc | 0.49095, 0.493\n",
      "i= 58800 train acc, test acc | 0.49151666666666666, 0.493\n",
      "i= 59400 train acc, test acc | 0.49161666666666665, 0.4929\n",
      "i= 60000 train acc, test acc | 0.4916833333333333, 0.4932\n",
      "i= 60600 train acc, test acc | 0.49175, 0.4931\n",
      "i= 61200 train acc, test acc | 0.4918666666666667, 0.4935\n",
      "i= 61800 train acc, test acc | 0.49188333333333334, 0.4926\n",
      "i= 62400 train acc, test acc | 0.4908166666666667, 0.4934\n",
      "i= 63000 train acc, test acc | 0.4920333333333333, 0.4933\n",
      "i= 63600 train acc, test acc | 0.49211666666666665, 0.4927\n",
      "i= 64200 train acc, test acc | 0.49246666666666666, 0.494\n",
      "i= 64800 train acc, test acc | 0.4924, 0.4938\n",
      "i= 65400 train acc, test acc | 0.49193333333333333, 0.4935\n",
      "i= 66000 train acc, test acc | 0.49246666666666666, 0.4941\n",
      "i= 66600 train acc, test acc | 0.49278333333333335, 0.4938\n",
      "i= 67200 train acc, test acc | 0.4925833333333333, 0.4935\n",
      "i= 67800 train acc, test acc | 0.4927166666666667, 0.4946\n",
      "i= 68400 train acc, test acc | 0.49238333333333334, 0.4938\n",
      "i= 69000 train acc, test acc | 0.49306666666666665, 0.4939\n",
      "i= 69600 train acc, test acc | 0.4925, 0.494\n",
      "i= 70200 train acc, test acc | 0.4930833333333333, 0.4946\n",
      "i= 70800 train acc, test acc | 0.49278333333333335, 0.4935\n",
      "i= 71400 train acc, test acc | 0.49351666666666666, 0.4938\n",
      "i= 72000 train acc, test acc | 0.49333333333333335, 0.4947\n",
      "i= 72600 train acc, test acc | 0.4932166666666667, 0.4952\n",
      "i= 73200 train acc, test acc | 0.4937, 0.4949\n",
      "i= 73800 train acc, test acc | 0.49346666666666666, 0.4953\n",
      "i= 74400 train acc, test acc | 0.4937166666666667, 0.4949\n",
      "i= 75000 train acc, test acc | 0.494, 0.4947\n",
      "i= 75600 train acc, test acc | 0.4939, 0.4944\n",
      "i= 76200 train acc, test acc | 0.4936333333333333, 0.4952\n",
      "i= 76800 train acc, test acc | 0.49385, 0.4941\n",
      "i= 77400 train acc, test acc | 0.49438333333333334, 0.4949\n",
      "i= 78000 train acc, test acc | 0.4941, 0.4953\n",
      "i= 78600 train acc, test acc | 0.49443333333333334, 0.4953\n",
      "i= 79200 train acc, test acc | 0.49443333333333334, 0.4948\n",
      "i= 79800 train acc, test acc | 0.49448333333333333, 0.4952\n",
      "i= 80400 train acc, test acc | 0.49423333333333336, 0.4955\n",
      "i= 81000 train acc, test acc | 0.49461666666666665, 0.4951\n",
      "i= 81600 train acc, test acc | 0.49475, 0.4947\n",
      "i= 82200 train acc, test acc | 0.49483333333333335, 0.4959\n",
      "i= 82800 train acc, test acc | 0.4948166666666667, 0.4958\n",
      "i= 83400 train acc, test acc | 0.49483333333333335, 0.4955\n",
      "i= 84000 train acc, test acc | 0.49495, 0.4953\n",
      "i= 84600 train acc, test acc | 0.4947666666666667, 0.495\n",
      "i= 85200 train acc, test acc | 0.49533333333333335, 0.4955\n",
      "i= 85800 train acc, test acc | 0.49545, 0.4959\n",
      "i= 86400 train acc, test acc | 0.49551666666666666, 0.4963\n",
      "i= 87000 train acc, test acc | 0.4953166666666667, 0.4962\n",
      "i= 87600 train acc, test acc | 0.49543333333333334, 0.4961\n",
      "i= 88200 train acc, test acc | 0.49548333333333333, 0.4963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 88800 train acc, test acc | 0.4956833333333333, 0.4969\n",
      "i= 89400 train acc, test acc | 0.49546666666666667, 0.4965\n",
      "i= 90000 train acc, test acc | 0.4957, 0.4964\n",
      "i= 90600 train acc, test acc | 0.4957166666666667, 0.4966\n",
      "i= 91200 train acc, test acc | 0.49605, 0.4971\n",
      "i= 91800 train acc, test acc | 0.49616666666666664, 0.4968\n",
      "i= 92400 train acc, test acc | 0.49605, 0.4967\n",
      "i= 93000 train acc, test acc | 0.4959, 0.4973\n",
      "i= 93600 train acc, test acc | 0.4960333333333333, 0.4973\n",
      "i= 94200 train acc, test acc | 0.49623333333333336, 0.4973\n",
      "i= 94800 train acc, test acc | 0.4963666666666667, 0.4968\n",
      "i= 95400 train acc, test acc | 0.49645, 0.4972\n",
      "i= 96000 train acc, test acc | 0.4961833333333333, 0.497\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Error BackPropagation Three-Layer NN (Numerical Differentiation)\n",
    "#################################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "  \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.x = None\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x) \n",
    "\n",
    "def relu_grad(x):\n",
    "    grad = np.zeros(x)\n",
    "    grad[x>=0] = 1\n",
    "    return grad\n",
    "    \n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None # softmax의 출력\n",
    "        self.t = None # 정답 레이블(원-핫 인코딩 형태)\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 떼\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arrrage(batch_size).self.t] -= 1\n",
    "            dx = dx/batch_size\n",
    "            \n",
    "        return dx\n",
    "    \n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim ==1:\n",
    "        t=t.reshape(1, t.size)\n",
    "        y=y.reshape(1, y.size)   \n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta)) / batch_size\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "class ThreeLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size_1)\n",
    "        self.params['b1'] = np.zeros(hidden_size_1)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size_1, hidden_size_2)\n",
    "        self.params['b2'] = np.zeros(hidden_size_2)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size_2, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        #계층 생성 \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        #self.layers['Relu1'] = Relu()\n",
    "        self.layers['Sigmoid1'] = Sigmoid()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['Relu3'] = Relu()\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x  = layer.forward(x)\n",
    "        \n",
    "        return(x)\n",
    "\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: \n",
    "            t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        \n",
    "        return grads\n",
    "   \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        #결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        \n",
    "        return grads\n",
    "\n",
    "# 데이터 읽기\n",
    "from mnist import load_mnist \n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=True, one_hot_label=True)\n",
    " \n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)\n",
    "\n",
    "\n",
    "network = ThreeLayerNet(input_size=784, hidden_size_1=300, hidden_size_2=50,  output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 100000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.01\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #print('x_batch =', x_batch.shape)   \n",
    "    #print('W1 =', network.params['W1'].shape)\n",
    "    \n",
    "    #noise = np.random.rand(100).astype(np.float32)\n",
    "    #np.zeros_like(noise)\n",
    "    \n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) #오차역전파법 방식 (훨씬 빠름)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"i=\", i, \"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
